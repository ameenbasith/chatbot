{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chatbot (Cosine Similarity)\n",
    "Below is the code implementing the Cosine Based Similarity portion of the chatbot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a20b36f612ce623"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:33:57.769427Z",
     "start_time": "2023-09-25T01:33:57.757700Z"
    }
   },
   "id": "7345d821f53af828"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "questions_list = df['Questions'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:33:58.449471Z",
     "start_time": "2023-09-25T01:33:58.445771Z"
    }
   },
   "id": "fdfa2f9b48ae7409"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "answers_list = df['Answers'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:33:59.315702Z",
     "start_time": "2023-09-25T01:33:59.309558Z"
    }
   },
   "id": "eba51aef16c35516"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import statements below\n",
    "nltk used for some nlp\n",
    "sklearn used for machine learning/data analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ce72d8cbaef3179"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ameen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ameen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ameen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:34:01.133765Z",
     "start_time": "2023-09-25T01:34:01.125374Z"
    }
   },
   "id": "a35c7e0186d7547d"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # non alpha-numeric\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]  # list comprehension to ONLY include tokens that are not stopwords!\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:37:11.305463Z",
     "start_time": "2023-09-25T01:37:11.303152Z"
    }
   },
   "id": "b4eb277afe165ec8"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def preprocess_with_stopwords(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # non alpha-numeric\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:37:12.492792Z",
     "start_time": "2023-09-25T01:37:12.487577Z"
    }
   },
   "id": "a4455324dc61a960"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/ameen/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "# X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"omw-1.4\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:37:14.665547Z",
     "start_time": "2023-09-25T01:37:13.789869Z"
    }
   },
   "id": "e86397b42d1a1ee8"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_text: data analyt\n",
      "Similarities: [[1.         0.32491362 0.39723569 0.3100033  0.3100033  0.50650236\n",
      "  0.37086918 0.37086918]]\n",
      "Max Similarity:  1.0000000000000002\n",
      "High Similarity Q's:  ['What is data analytics?']\n",
      "['Data analytics is the process of examining raw data to discover meaningful patterns, draw conclusions, and make informed decisions. It involves collecting, transforming, and analyzing large sets of data to extract valuable insights and support decision-making.']\n",
      "Processed Text w Stopwords:  what is data analyt\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Data analytics is the process of examining raw data to discover meaningful patterns, draw conclusions, and make informed decisions. It involves collecting, transforming, and analyzing large sets of data to extract valuable insights and support decision-making.'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
    "\n",
    "def get_response(text):\n",
    "    processed_text = preprocess(text)\n",
    "    print(\"processed_text:\", processed_text)\n",
    "    vectorized_text = vectorizer.transform([processed_text])\n",
    "    similarities = cosine_similarity(vectorized_text, X)\n",
    "    print(\"Similarities:\", similarities)\n",
    "    max_similarity = np.max(similarities)\n",
    "    print(\"Max Similarity: \", max_similarity)\n",
    "    if max_similarity > 0.6:\n",
    "        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]\n",
    "        print(\"High Similarity Q's: \", high_similarity_questions)\n",
    "        \n",
    "        target_answers = []\n",
    "        for q in high_similarity_questions:\n",
    "            q_index = questions_list.index(q)\n",
    "            target_answers.append(answers_list[q_index])\n",
    "        print(target_answers)\n",
    "        \n",
    "        Z = vectorizer.fit_transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
    "        processed_text_with_stopwords = preprocess_with_stopwords(text)\n",
    "        print(\"Processed Text w Stopwords: \", processed_text_with_stopwords)\n",
    "        vectorized_text_with_stopwords = vectorizer.transform([processed_text_with_stopwords])\n",
    "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
    "        closest = np.argmax(final_similarities)\n",
    "        return target_answers[closest]\n",
    "    else:\n",
    "        return \"I can't answer this question\"\n",
    "    \n",
    "get_response('What is data analytics?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:37:17.428041Z",
     "start_time": "2023-09-25T01:37:16.378996Z"
    }
   },
   "id": "f1b67165ce112f85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "52c530fb65ccb1c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
